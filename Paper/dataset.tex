\section{Data Set and Feature Selection}
%\cite*{mimno-mccallum-11}

\subsection{Dataset Construction}
\indent
The United States Patent Office makes bulk patent grant information available through several private companies\cite{USPTO:2013:patent-catalog} in XML format\cite{USPTO:2013:dtd}. For our experiments we drew samples from patent grants made between May 1st 2012 and July 31st 2012. Over these three months the USPTO granted a total of 76,317 patents, or roughly 6,000 per week. We constructed three datasets from this, \emph{even-3500}, \emph{jagged-20000}, and \emph{jagged-40000}. \emph{even-3500} consisted of 350 training and 150 test instances from 7 of the 8 IPC patent classes (one class, D, which corresponds to textiles and paper could not be used since a only 47 patents were granted to that section in our dataset). \emph{jagged-20000} and \emph{jagged-40000} were simply a random sample of 20,000 and 40,000 patents respectively granted within the given timeframe, using all 8 of the IPC patent classes.

\subsection{Feature Selection}
\indent
Patents are semi-structured. That is, rather than simply being free text, there are certain fields that all patent grants are guaranteed to have (as well as other, optional fields). This needs to be taken into consideration when designing features for patent classification. 

Although a domain expert could potentially design features that were powerful, we ended up considering three relatively general feature sets, \emph{description-unigrams}, \emph{abstract-bigrams}, and \emph{tf-idf}. \emph{description-unigrams} consists of unigrams from the main ``description'' field of the patent document, tokenized and with stopwords removed. \emph{abstract-bigrams} consists of unigrams and bigrams from the ``abstract'', ``claims'', and ``title'' fields of the patent. \emph{tf-idf} consists of the tokens (and token-pairs) from \emph{abstract-bigrams} weighted them with a statistic called term frequency inverse document frequency (tf-idf)\cite{manning:2008:IR}. Intuitively, the tf-idf weight of a token (or token bigram) in a document increases as the number of times it occurs in that document increases and decreases as the number of times it appears in every other document increases. This weighting gives us a sense of how characteristic the token (or token bigram) is of each document. Once we calculated tf-idf scores for each document, we truncated the word bags to contain only the 32, 100, and 250 highest weighted features for each patent. We used tools in FACTORIE\cite{mccallum09:factorie:} to tokenize the patents and remove stopwords. We also added domain-specific stopwords based on observations of patent documents. 

Both of these bag of n-grams models can be easily translated into vectors. This allows us to use a variety of standard machine learning multiclass classification techniques. For some set of documents $\mathbf{D}$, let $| \mathbf{V} |$ denote the number of unique n-grams that appear in that set of documents (for a given tokenizer and stopword lexicon). Then we can represent each document $ d \in \mathbf{D}$ by a binary vector $f \in \{0,1\}^{| \mathbf{V} |}$, where $f^d_i = 1$ when $d$ contains the $i$\textsuperscript{th} n-gram in the vocabulary and 0 otherwise. Nominally, these feature vectors are very large ($| \mathbf{V} | > 900000$ for \emph{abstract-bigrams} on \emph{jagged-40000}). Thus we need to represent these vectors sparsely in practice in order to work tractably with them.


%Our data is from the United States Patent and Trademark Office Bulk Downloads located on Google.  The original data file is prepared as an XML file, which we then parse to find a list of words contained within the patent.  Based on the total list of words of all patents in a sample, a feature vector is created such that the feature vector includes a feature for every seen word within the dataset.  A 1 is included for a feature within a feature vector for a patent if the patent contains the word, and a 0 is included for a specific feature if the patent did not include the word.  This bag of words technique was used for all experiments.