\section{Classification Techniques}
%%Just have dataset here instead of a whole separate section? If it's in it's own section then we can give a fuller background on the classes and maybe an explanation of the coherence scores
%\subsection{Data Set}

\subsection{Maximum Entropy} % (fold)
\label{sub:maximum_entropy}

\indent
Maximum entropy models are standard tools in document classification\cite{Nigam99usingmaximum}. A maximum entropy classifier calculates a probability distribution over label classes ($c$) conditioned on the observed feature vector. Usually the predicted class of a feature vector is taken to be the class with the highest probability under a learned parameterization of the distribution. The learning step simply involves estimating the natural parameters of an exponential distribution. Customarily the observed feature vector is binary. Taking $f(c,d)$ to represent the feature vector for a single document ($d$) being assigned to a given class, and $\theta$ to be the weight vector we want to learn, this gives
\begin{align*}
	P(c|f, \theta) = \frac{e^{\theta \cdot f(c, d)}}{\sum_{c'} e^{\theta \cdot f(c',d)}}
\end{align*}

We can estimate the parameters of this distribution easily using standard convex optimization techniques. Since this distribution is a natural formulation of the exponential family, it is the distribution that highest entropy subject to the constraints imposed by the observed feature vectors, hence its name. 

In our case where the feature vector is sparse and of very high dimensionality, it is necessary to regularize the learned weight vector $\theta$. Regularization allows us to avoid issues that arise when learned parameter weights would grow too large or be very small. If a given feature only appears in one class of the training data, an unregularized system will learn an infinite weight on it, since it perfectly predicts that class. Conversely, if a feature is a very weak predictor the regularization will push the weight to zero. This acts effectively as an online dimensionality reduction. As is customary, we used a regularization technique that conceptually corresponds to putting a standard normal prior on the learned weights. For these experiments we used the Maximum Entropy model implemented in FACTORIE, which uses LBFGS\cite{Liu89onthe} to perform the gradient descent necessary to estimate parameters.

% subsection maximum_entropy (end)

\subsection{Multi-class SVM} % (fold)
\label{sub:multiclass_svm}

Given a sequence of training examples $(c, x)$ where $c \in \mathbf{C}$ and $x = f(\cdot, d)$ for each $d \in \mathbf{D}$. We define binary functions $y_c(c
') = \twopartdef{1}{ c' = c}{0}{otherwise}$. For multi-class classification we train $| \mathbf{C} |$ binary classifiers. For each class we train a support vector machine by solving a constrained minimization problem for the weight vector:
\begin{align*}
	\min_{w,\xi} & \left( \frac{1}{2} \| \mathbf{w} \|^2 + C \sum_{i=1}^n \xi_i \right) \\
	\text{subject to} &  \text{ } \forall i, \text{ } y_i(w \cdot x_i - b) > 1 - \xi_i, \text{ } \xi_i > 0
\end{align*}

There are a number of black-box SVM implementations that can efficiently train an SVM with multiple different parameter settings. We used LIBSVM\cite{CC01a} to run our SVM experiments. The implementation uses a sequential minimum optimization technique\cite{Fan05workingset} to efficiently minimize $w$. We used linear kernels for all of our experiments, as they have proven useful for document classification in the past\cite{DBLP:conf/ecml/Joachims98}.


%All of our experiments were run with a linear kernel at the default settings. % WHAT ARE THEY?


% subsection multiclass_svm (end)


%\subsection{Supervised Methods}
%For the first experiment, the dataset was created such that each of the 7 classification categories were evenly represented with 200? samples each.  (Define Training/Testing and re-run experiment)

%Description of SVM -------



%% Tempted to make this it's own section as well
\subsection{Dimensionality Reduction}
There are several informal dimensionality reduction techniques that we have used to compress our very high dimensional space into a slightly smaller space. The removal of stopwords can be seen as a very basic dimensionality reduction. Similarly, tf-idf is an unsupervised dimensionality reduction technique that removes dimensions that are likely to have little predictive power.  
\\
\\In addition to these we attempted to use two more traditional dimensionality techniques, principle component analysis (PCA), and Laplacian eigenmap.  For our purposes, we used a MATLAB script titled mani.m by Todd Wittmann \cite{Wittman}.   Saul et al. \cite{saul06spectral} provide an intuitive explanation of PCA that is very useful for understanding it.  They state PCA can be viewed as trying to find a set of features which is able to replicate the original variance structure.  During dimensionality reduction, the features are ordered such that the features which maximize the variance are selected first.  They further state that the Laplacian Eigenmap allows for local mappings between inputs to be maintained.  These intuitive descriptions are useful for understanding the results found in our experimental sections.

%Description of PCA and Laplacian


%For the second experiment, the dataset from experiment one had two different dimensionality reduction techniques used to reduce the dataset.  The two included PCA and a Laplacian Eigenmap.  (Define Training/Testing and re-run experiment)

%\subsection{Boosting Methods}
%For the third experiment, we attempted to determine if more data would noticeably improve the results of the algorithm.  Specifically, the dataset was expanded to approximately 2000 samples each. (Define Training/Testing and re-run experiment)